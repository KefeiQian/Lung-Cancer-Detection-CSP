{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhd_info_csv_path = \"data/mhd_info.csv\"\n",
    "mhd_info_df = pd.read_csv(mhd_info_csv_path)\n",
    "mhd_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_included_path = \"data/annotations.csv\"\n",
    "annotations_included_df = pd.read_csv(annotations_included_path, delimiter=\",\")\n",
    "annotations_included_df[\"included\"] = 1\n",
    "annotations_included_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_excluded_path = \"data/annotations_excluded.csv\"\n",
    "annotations_excluded_df = pd.read_csv(annotations_excluded_path, delimiter=\",\")\n",
    "annotations_excluded_df[\"included\"] = 0\n",
    "annotations_excluded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.concat([annotations_included_df, annotations_excluded_df])\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_detection_result():\n",
    "    # detection_result_folder = \"../output/valresults/caltech/h/off\"\n",
    "    detection_result_folder = \"../output/valresults/caltech/hw/off\"\n",
    "\n",
    "    detection_dfs = {}\n",
    "    for epoch_id in tqdm(os.listdir(detection_result_folder), desc=\"test epoch\"):\n",
    "        current_folder_path = \"{}/{}\".format(detection_result_folder, epoch_id)\n",
    "\n",
    "        detection_dfs.setdefault(epoch_id, {})\n",
    "        for filename in os.listdir(current_folder_path):\n",
    "            series_id = filename[0:-4]\n",
    "            file = os.path.join(current_folder_path, filename)\n",
    "            try:\n",
    "                result_df = pd.read_csv(file, delimiter=\" \", header=None)\n",
    "                result_df.columns = [\"z-index\", \"top_left_x\", \"top_left_y\", \"width\", \"height\", \"probability\"]\n",
    "                detection_dfs.get(epoch_id).setdefault(series_id, result_df)\n",
    "            except Exception:\n",
    "                # print(epoch_id, filename, \"skipped\")\n",
    "                empty_df = pd.DataFrame()\n",
    "                detection_dfs.get(epoch_id).setdefault(series_id, empty_df)\n",
    "\n",
    "    return detection_dfs\n",
    "\n",
    "test_detections = read_detection_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nodule_included(n):\n",
    "    return n[\"included\"] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_froc():\n",
    "    for epoch_id in tqdm(list(test_detections.keys())[82:90]):\n",
    "    #for epoch_id in [\"028\"]:\n",
    "        epoch_candidates = test_detections[epoch_id]\n",
    "\n",
    "        total_number_of_candidates = 0\n",
    "        total_number_of_nodules = 0\n",
    "        irrelevant_candidates = 0\n",
    "\n",
    "        double_candidates_ignored = 0\n",
    "\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tn = 0\n",
    "\n",
    "        min_probability = -1\n",
    "\n",
    "        froc_gts = []\n",
    "        froc_probs = []\n",
    "\n",
    "        fp_divisors = []\n",
    "        exclude_list = []\n",
    "\n",
    "        ignored_candidates = []\n",
    "\n",
    "        series_ids = epoch_candidates.keys()\n",
    "\n",
    "        for series_id in tqdm(series_ids):\n",
    "            mhd_info = mhd_info_df[mhd_info_df[\"series_id\"] == series_id].iloc[0]\n",
    "            origins = mhd_info[\"origins\"]\n",
    "            spacings = mhd_info[\"spacings\"]\n",
    "            is_flip = mhd_info[\"is_flip\"]\n",
    "\n",
    "            spacings = json.loads(spacings)\n",
    "            origins = json.loads(origins)\n",
    "\n",
    "            [z_spacing, y_spacing, x_spacing] = spacings\n",
    "            [z_origin, y_origin, x_origin] = origins\n",
    "\n",
    "            # candidates = epoch_candidates[series_id].reset_index().query(\"probability >= {}\".format(probability_threshold))\n",
    "            candidates = epoch_candidates[series_id].reset_index()\n",
    "            # top_z_index = set(candidates.sort_values(by=['probability'], ascending=False)[0:20][\"z-index\"].values)\n",
    "            top_z_index = set(candidates[candidates[\"probability\"] > probability_threshold][\"z-index\"].values)\n",
    "            candidates = candidates[candidates[\"z-index\"].isin(top_z_index)].sort_values(by=['probability'], ascending=False)\n",
    "            candidates2 = candidates.copy()\n",
    "            total_number_of_candidates = len(candidates)\n",
    "\n",
    "            nodules = annotations[annotations[\"seriesuid\"] == series_id]\n",
    "\n",
    "            for _, nodule in nodules.iterrows():\n",
    "                if is_nodule_included(nodule):\n",
    "                    total_number_of_nodules += 1\n",
    "\n",
    "                x = nodule[\"coordX\"]\n",
    "                y = nodule[\"coordY\"]\n",
    "                z = nodule[\"coordZ\"]\n",
    "                diameter = nodule[\"diameter_mm\"]\n",
    "\n",
    "                if diameter <= 0:\n",
    "                    diameter = 10\n",
    "\n",
    "                radius_squared = pow((diameter / 2), 2)\n",
    "\n",
    "                matched_candidates = []\n",
    "\n",
    "                for candidate_id, candidate in candidates.iterrows():\n",
    "                    x2 = candidate[\"top_left_x\"]\n",
    "                    y2 = candidate[\"top_left_y\"]\n",
    "                    z2 = candidate[\"z-index\"]\n",
    "\n",
    "                    if is_flip:\n",
    "                        x2 = image_size[0] - x2\n",
    "                        y2 = image_size[1] - y2\n",
    "\n",
    "                    x2 = x2 * x_spacing + x_origin\n",
    "                    y2 = y2 * y_spacing + y_origin\n",
    "                    z2 = z2 * z_spacing + z_origin\n",
    "\n",
    "                    dist = pow(x - x2, 2) + pow(y - y2, 2) + pow(z - z2, 2)\n",
    "                    if dist < radius_squared:\n",
    "                        if is_nodule_included(nodule):\n",
    "                            matched_candidates.append(candidate)\n",
    "                            # if candidate_id not in candidates2.index.values:\n",
    "                            #     print(candidate, nodule)\n",
    "                            # else:\n",
    "                            #     candidates2.drop([candidate_id])\n",
    "                        # else:\n",
    "                        #     if candidate_id in candidates2.index.values:\n",
    "                        #         irrelevant_candidates += 1\n",
    "                        #         ignored_candidates.append(nodule)\n",
    "                        #         candidates2.drop([candidate_id])\n",
    "                        break\n",
    "\n",
    "                # if len(matched_candidates) > 1:\n",
    "                #     double_candidates_ignored += (len(matched_candidates) - 1)\n",
    "\n",
    "                if is_nodule_included(nodule):\n",
    "                    if len(matched_candidates) > 0:\n",
    "                        # max_probability = min_probability\n",
    "                        # for candidate in matched_candidates:\n",
    "                        #     max_probability = max(max_probability, candidate[\"probability\"])\n",
    "                        #\n",
    "                        # froc_gts.append(1)\n",
    "                        # froc_probs.append(max_probability)\n",
    "                        # fp_divisors.append(series_id)\n",
    "                        # exclude_list.append(False)\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fn += 1\n",
    "                        # froc_gts.append(1)\n",
    "                        # froc_probs.append(min_probability)\n",
    "                        # fp_divisors.append(series_id)\n",
    "                        # exclude_list.append(True)\n",
    "\n",
    "            fp += len(candidates2)\n",
    "\n",
    "            # for _, candidate3 in candidates2.iterrows():\n",
    "            #     fp += 1\n",
    "            #     froc_gts.append(0)\n",
    "            #     froc_probs.append(candidate3[\"probability\"])\n",
    "            #     fp_divisors.append(series_id)\n",
    "            #     exclude_list.append(False)\n",
    "\n",
    "        print(epoch_id)\n",
    "        print(\"tp\", tp)\n",
    "        print(\"fp\", fp)\n",
    "        print(\"fn\", fn)\n",
    "        # print(\"tn\", tn)\n",
    "        #\n",
    "        # print(\"total number of candidates\", total_number_of_candidates)\n",
    "        # print(\"total number of nodules\", total_number_of_nodules)\n",
    "        #\n",
    "        # print(\"irrelevant candidates\", irrelevant_candidates)\n",
    "        # print(\"Ignored candidates which were double detections on a nodule\", double_candidates_ignored)\n",
    "\n",
    "        sensitivity_x.append(int(epoch_id))\n",
    "        if total_number_of_nodules == 0:\n",
    "            print(\"sensitivity\", 0)\n",
    "            sensitivity_y.append(0)\n",
    "        else:\n",
    "            print(\"sensitivity\", tp / total_number_of_nodules)\n",
    "            sensitivity_y.append(tp / total_number_of_nodules)\n",
    "\n",
    "        print(\"Average number of candidates per scan\", total_number_of_candidates / len(series_ids))\n",
    "        fps.append(fp)\n",
    "\n",
    "        # y = []\n",
    "        # y_prob = []\n",
    "        #\n",
    "        # for i in range(len(exclude_list)):\n",
    "        #     y.append(froc_gts[i])\n",
    "        #     y_prob.append(froc_probs[i])\n",
    "        #\n",
    "        # fpr, tpr, thresholds = metrics.roc_curve(y, y_prob)\n",
    "        #\n",
    "        # #create ROC curve\n",
    "        # plt.plot(fpr, tpr)\n",
    "        # plt.ylabel('True Positive Rate')\n",
    "        # plt.xlabel('False Positive Rate')\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_x = []\n",
    "sensitivity_y = []\n",
    "\n",
    "fps = []\n",
    "\n",
    "probability_threshold = 0.1\n",
    "\n",
    "evaluate_froc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(25, 5))\n",
    "plt.plot(sensitivity_x, sensitivity_y, label='Sensitivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(25, 5))\n",
    "plt.plot(sensitivity_x, fps, label='fp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "sorted(list(zip(sensitivity_x, sensitivity_y, fps)), key=lambda t: t[1]*10000-math.log(t[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(\"Number of cpu : \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
