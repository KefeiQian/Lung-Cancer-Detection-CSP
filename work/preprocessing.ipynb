{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import SimpleITK as sitk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_subset_id = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_folder = \"../downloads\"\n",
    "download_data_folder = \"../downloads/data\"\n",
    "download_mask_folder = \"../downloads/seg-lungs-LUNA16\"\n",
    "\n",
    "output_nodules_folder = \"output/nodules\"\n",
    "\n",
    "data_image_folder = \"../data/images\"\n",
    "\n",
    "gt_cache_path = \"../data/cache/train_gt\"\n",
    "no_gt_cache_path = \"../data/cache/train_no_gt\"\n",
    "test_cache_path = \"../data/cache/test\"\n",
    "\n",
    "mhd_info_csv_path = \"data/mhd_info.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in [output_nodules_folder, data_image_folder]:\n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "image_size = (512, 512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotations_path = \"../downloads/annotations.csv\"\n",
    "annotations_df = pd.read_csv(annotations_path, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "candidates_path = \"../downloads/candidates_V2.csv\"\n",
    "candidates_df = pd.read_csv(candidates_path, delimiter=\",\")\n",
    "gt_candidates_df = candidates_df[candidates_df[\"class\"] == 1]\n",
    "no_gt_candidates_df = candidates_df[candidates_df[\"class\"] != 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    image[image > 400] = 400\n",
    "    image[image < -1000] = -1000\n",
    "\n",
    "    max_hu = image.max()\n",
    "    min_hu = image.min()\n",
    "    image = (image - min_hu) / (max_hu - min_hu) * 255\n",
    "    image = np.round(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def world_to_voxel_coord(world_coord, origin, spacing):\n",
    "    stretched_voxel_coord = np.absolute(world_coord - origin)\n",
    "    voxel_coord = stretched_voxel_coord / spacing\n",
    "    return voxel_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def extract_mhd_file_info(subset_id, series_id):\n",
    "    mhd_file_path = \"{}/subset{}/{}.mhd\".format(download_data_folder, subset_id, series_id)\n",
    "    mask_file_path = \"{}/{}.mhd\".format(download_mask_folder, series_id)\n",
    "\n",
    "    itk_image = sitk.ReadImage(mhd_file_path, sitk.sitkFloat32)\n",
    "    ct_scans = sitk.GetArrayFromImage(itk_image)\n",
    "\n",
    "    origins = np.array(list(reversed(itk_image.GetOrigin())))\n",
    "    spacings = np.array(list(reversed(itk_image.GetSpacing())))\n",
    "\n",
    "    direction = itk_image.GetDirection()\n",
    "    direction = np.array(list(map(lambda x: round(x), direction)))\n",
    "\n",
    "    if np.any(direction != np.array([1, 0, 0, 0, 1, 0, 0, 0, 1])):\n",
    "        is_flip = True\n",
    "    else:\n",
    "        is_flip = False\n",
    "\n",
    "    ct_image_folder = \"{}/{}\".format(data_image_folder, series_id)\n",
    "\n",
    "    mhd_info = {\n",
    "        \"mhd_file_path\": mhd_file_path,\n",
    "        \"mask_file_path\": mask_file_path,\n",
    "        \"subset_id\": subset_id,\n",
    "        \"series_id\": series_id,\n",
    "        \"total_images\": ct_scans.shape[0],\n",
    "        \"origins\": origins.tolist(),\n",
    "        \"spacings\": spacings.tolist(),\n",
    "        \"is_flip\": is_flip,\n",
    "        \"ct_image_folder\": ct_image_folder\n",
    "    }\n",
    "\n",
    "    return mhd_info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def extract_ct_images(mhd_info, mask=False):\n",
    "    mhd_file_path = mhd_info[\"mhd_file_path\"]\n",
    "    mask_file_path = mhd_info[\"mask_file_path\"]\n",
    "    is_flip = mhd_info[\"is_flip\"]\n",
    "    total_images = mhd_info[\"total_images\"]\n",
    "    ct_image_folder = mhd_info[\"ct_image_folder\"]\n",
    "\n",
    "    ct_scans = sitk.GetArrayFromImage(sitk.ReadImage(mhd_file_path, sitk.sitkFloat32))\n",
    "    if mask:\n",
    "        masks = sitk.GetArrayFromImage(sitk.ReadImage(mask_file_path, sitk.sitkFloat32))\n",
    "\n",
    "    if is_flip:\n",
    "        ct_scans = ct_scans[:, ::-1, ::-1]\n",
    "        if mask:\n",
    "            masks = masks[:, ::-1, ::-1]\n",
    "\n",
    "    ct_scans = normalize(ct_scans)\n",
    "    ct_scans = ct_scans.astype(np.uint8)\n",
    "    if mask:\n",
    "        masks = masks.astype(np.uint8)\n",
    "\n",
    "    if not os.path.exists(ct_image_folder):\n",
    "        os.makedirs(ct_image_folder)\n",
    "\n",
    "    for z in range(0, total_images):\n",
    "        if mask:\n",
    "            masked_image = cv2.bitwise_and(ct_scans[z], ct_scans[z], mask=masks[z])\n",
    "            colored_image = cv2.cvtColor(masked_image, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            colored_image = cv2.cvtColor(ct_scans[z], cv2.COLOR_GRAY2RGB)\n",
    "        cv2.imwrite(\"{}/z-{}.jpg\".format(ct_image_folder, z), colored_image)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def generate_train_test_cache(mhd_info):\n",
    "    subset_id = mhd_info[\"subset_id\"]\n",
    "    series_id = mhd_info[\"series_id\"]\n",
    "    origins = mhd_info[\"origins\"]\n",
    "    spacings = mhd_info[\"spacings\"]\n",
    "    is_flip = mhd_info[\"is_flip\"]\n",
    "    total_images = mhd_info[\"total_images\"]\n",
    "    ct_image_folder = mhd_info[\"ct_image_folder\"]\n",
    "\n",
    "    spacings = np.array(json.loads(spacings))\n",
    "    origins = np.array(json.loads(origins))\n",
    "\n",
    "    annotations = annotations_df[annotations_df[\"seriesuid\"] == series_id]\n",
    "\n",
    "    nodules_annotation = []\n",
    "    for _, row in annotations.iterrows():\n",
    "        world_x = row[\"coordX\"]\n",
    "        world_y = row[\"coordY\"]\n",
    "        world_z = row[\"coordZ\"]\n",
    "        diameter = row[\"diameter_mm\"]\n",
    "\n",
    "        world_coord = np.array([float(world_z), float(world_y), float(world_x)])\n",
    "        voxel_coord = world_to_voxel_coord(world_coord, origins, spacings)\n",
    "\n",
    "        voxel_z, voxel_y, voxel_x = voxel_coord\n",
    "        if is_flip:\n",
    "            voxel_x = image_size[0] - voxel_x\n",
    "            voxel_y = image_size[1] - voxel_y\n",
    "\n",
    "        radius = diameter / spacings[2] / 2\n",
    "\n",
    "        annotation = {\"z\": round(voxel_z), 'bbox':\n",
    "            np.array([\n",
    "                max(round(voxel_x - radius), 0),\n",
    "                max(round(voxel_y - radius), 0),\n",
    "                min(round(voxel_x + radius), image_size[0] - 1),\n",
    "                min(round(voxel_y + radius), image_size[1] - 1)\n",
    "            ]), 'ignoreareas': np.array([])}\n",
    "        nodules_annotation.append(annotation)\n",
    "    nodules_df = pd.DataFrame(nodules_annotation)\n",
    "\n",
    "    no_gt_df = no_gt_candidates_df[no_gt_candidates_df[\"seriesuid\"] == series_id]\n",
    "    no_gt_z = set()\n",
    "    for _, row in no_gt_df.iterrows():\n",
    "        world_x = row[\"coordX\"]\n",
    "        world_y = row[\"coordY\"]\n",
    "        world_z = row[\"coordZ\"]\n",
    "\n",
    "        world_coord = np.array([float(world_z), float(world_y), float(world_x)])\n",
    "        voxel_coord = world_to_voxel_coord(world_coord, origins, spacings)\n",
    "\n",
    "        voxel_z, voxel_y, voxel_x = voxel_coord\n",
    "        no_gt_z.add(round(voxel_z))\n",
    "\n",
    "    for z in range(0, total_images):\n",
    "        filepath = \"{}/z-{}.jpg\".format(ct_image_folder[3:], z)\n",
    "        bboxes = []\n",
    "        ignore_areas = np.array([])\n",
    "\n",
    "        if len(nodules_annotation) != 0:\n",
    "            for _, nodule in nodules_df[nodules_df[\"z\"] == z].iterrows():\n",
    "                bboxes.append(nodule[\"bbox\"])\n",
    "\n",
    "        anno = {'bboxes': np.array(bboxes), 'ignoreareas': ignore_areas,\n",
    "                'filepath': filepath}\n",
    "\n",
    "        if len(bboxes) != 0:\n",
    "            if subset_id != test_subset_id:\n",
    "                image_data_gt.append(anno)\n",
    "            else:\n",
    "                image_data_test.append(anno)\n",
    "        else:\n",
    "            if z in no_gt_z:\n",
    "                image_data_no_gt.append(anno)\n",
    "\n",
    "            if subset_id == test_subset_id:\n",
    "                image_data_test.append(anno)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "subset: 100%|██████████| 10/10 [02:04<00:00, 12.41s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_image_folder):\n",
    "    os.makedirs(data_image_folder)\n",
    "\n",
    "image_data_gt, image_data_no_gt, image_data_test = [], [], []\n",
    "\n",
    "if os.path.exists(mhd_info_csv_path):\n",
    "    mhd_info_df = pd.read_csv(mhd_info_csv_path)\n",
    "else:\n",
    "    mhd_infos = []\n",
    "\n",
    "for subset_id in tqdm(range(0, 10), desc=\"subset\"):\n",
    "    current_folder = os.listdir(\"{}/subset{}\".format(download_data_folder, subset_id))\n",
    "\n",
    "    for file in current_folder:\n",
    "        if file.endswith(\".mhd\"):\n",
    "            series_id = re.search(r'(.*)\\.mhd', file).group(1)\n",
    "            if os.path.exists(mhd_info_csv_path):\n",
    "                mhd_info = mhd_info_df[mhd_info_df[\"series_id\"] == series_id].iloc[0]\n",
    "            else:\n",
    "                mhd_info = extract_mhd_file_info(subset_id, series_id)\n",
    "                mhd_infos.append(mhd_info)\n",
    "\n",
    "            # extract_ct_images(mhd_info, mask=True)\n",
    "            generate_train_test_cache(mhd_info)\n",
    "\n",
    "if not os.path.exists(mhd_info_csv_path):\n",
    "    mhd_info_df = pd.DataFrame(mhd_infos)\n",
    "    mhd_info_df.to_csv(mhd_info_csv_path, index=None)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_data_gt_df = pd.DataFrame(image_data_gt)\n",
    "image_data_no_gt_df = pd.DataFrame(image_data_no_gt)\n",
    "image_data_test_df = pd.DataFrame(image_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "image_data_gt_df.set_index(\"filepath\", inplace=True)\n",
    "image_data_no_gt_df.set_index(\"filepath\", inplace=True)\n",
    "image_data_test_df.set_index(\"filepath\", inplace=True)\n",
    "\n",
    "image_data_gt_df.to_csv(\"output/gt.csv\")\n",
    "image_data_no_gt_df.to_csv(\"output/no_gt.csv\")\n",
    "image_data_test_df.to_csv(\"output/test.csv\")\n",
    "with open(gt_cache_path, 'wb') as fid:\n",
    "    pickle.dump(image_data_gt, fid, 2)\n",
    "with open(no_gt_cache_path, 'wb') as fid:\n",
    "    pickle.dump(image_data_no_gt, fid, 2)\n",
    "with open(test_cache_path, 'wb') as fid:\n",
    "    pickle.dump(image_data_test, fid, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lung-Cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d4e508bcdf8c60a886d154242a7d048eb1090ffddac0cfb1f3922d8edcb8935"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
